{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备语料\n",
    "- 以TECCL语料库为例\n",
    "- Ten-thousand English Compositions of Chinese Learners (the TECCL Corpus)，中国学生万篇英语作文语料库V1.1，由北外许家金教授创建的开放获取学习者语料库。网址：http://corpus.bfsu.edu.cn/content/teccl-corpus\n",
    "- 下载后解压\n",
    "- 把其中的“01TECCL_V1.1_RAW”文件夹包括其中的文件拷贝到当前目录\n",
    "- **注意** 这个语料的规模对于词向量训练是不够的，仅做演示之用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"red\",size=4>**只需在这部分设置参数、语料地址、模型保存地址以及语料库预处理定制**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量参数\n",
    "- 根据需要修改下面的参数，注意保留每一行后面的英文逗号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>size</b>:  Dimensionality of the word vectors. 向量维度\n",
    "- <b>window</b>: Maximum distance between the current and predicted word within a sentence.窗口大小\n",
    "- <b>min_count</b>: Ignores all words with total frequency lower than this. 最小词频\n",
    "- <b>iter</b>: Number of iterations (epochs) over the corpus. 训练次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = {\n",
    "     \"size\" : 100,\n",
    "     \"window\" : 5,\n",
    "    \"min_count\" : 5,\n",
    "    \"iter\": 5,   \n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料地址以及词向量模型保存地址\n",
    "- 文件或目录地址要用反斜杠“/”分割路径\n",
    "- 放在英文模式引号里面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 语料地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"01TECCL_V1.1_RAW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 词向量模型保存地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = \"teccl.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料预处理设置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否分句： 是， True; 否, False\n",
    "- 如果语料是每行一句，一般可以选否；否则根据需要是否将每一个段落进行分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否分词： 是， True; 否, False\n",
    "- 如果语料中每个单词及标点已经由空格分隔，选否；否则根据需要是否将每个句子进行分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否统一为小写： 是，True； 否，False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否去除停用词： 是， True; 否, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 默认调用nltk的停用词列表，即None\n",
    "- 若需要更改，请填入停用词列表， 如：\n",
    "```python\n",
    "stopwords = ['is','a','the','an']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import nltk\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):    \n",
    "    def __init__(self,corp_dir = None,suffixes =None,**kwargs):\n",
    "        self.fdir = corp_dir\n",
    "        suffixes = [suffixes] if isinstance(suffixes,str) else suffixes\n",
    "        self.suffixes = suffixes if suffixes else ['.txt']\n",
    "        self.files = self._get_files(self.fdir)\n",
    "        self.kwargs = kwargs\n",
    "        if kwargs['remove_stopwords']:\n",
    "            if not kwargs['stopwords']:\n",
    "                from nltk.corpus import stopwords\n",
    "                try:\n",
    "                    self.stopwords = stopwords.words('english')\n",
    "                except:\n",
    "                    nltk.download('stopwords')\n",
    "                    self.stopwords = stopwords.words('english')\n",
    "            else:\n",
    "                self.stopwords = kwargs['stopwords']\n",
    "        \n",
    "    def _get_files(self,path):\n",
    "        files = []\n",
    "        for f in os.listdir(path):\n",
    "            fpath = os.path.join(path,f)\n",
    "            if os.path.isdir(fpath): continue            \n",
    "            if self._check_file_type(f):\n",
    "                files.append(fpath)\n",
    "        logging.info('%d file(s) loaded!'%len(files))\n",
    "        return files\n",
    "                    \n",
    "    def _check_file_type(self,f):\n",
    "        for suffix in self.suffixes:\n",
    "            if f.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def __iter__(self):\n",
    "        for f in self.files:\n",
    "            for line in open(f,encoding='utf-8'):\n",
    "                line = line.strip()                \n",
    "                if not line: continue                          \n",
    "                for words in self.preprocess(line):\n",
    "                    yield words\n",
    "                    \n",
    "    def preprocess(self,line):\n",
    "        sents = nltk.tokenize.sent_tokenize(line) if self.kwargs['sent_tokenize'] else [line]\n",
    "        for sent in sents:\n",
    "            words = nltk.tokenize.word_tokenize(sent) if self.kwargs['word_tokenize'] else sent.split()\n",
    "            out_words = []\n",
    "            for w in words:\n",
    "                if not w.strip(): continue  \n",
    "                if self.kwargs['lower_case']:\n",
    "                    w = w.lower()\n",
    "                if self.kwargs['remove_stopwords']:\n",
    "                    if w in self.stopwords:\n",
    "                        continue\n",
    "                out_words.append(w)\n",
    "            \n",
    "            yield out_words\n",
    "\n",
    "\n",
    "def train_w2vmodel(sentences,save2path,**kwargs):     \n",
    "    model = gensim.models.Word2Vec(**kwargs)   \n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences,total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.save(save2path,ignore=[])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 21:30:59,046 : INFO : 9864 file(s) loaded!\n"
     ]
    }
   ],
   "source": [
    "mycorpus = Corpus(corpus_path,suffixes=['.txt'],\n",
    "                          sent_tokenize = sent_tokenize,\n",
    "                         word_tokenize = word_tokenize,\n",
    "                         lower_case = lower_case,\n",
    "                         remove_stopwords = remove_stopwords,\n",
    "                         stopwords= stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"red\",size=3>**启动词向量训练**</font>\n",
    "- 若已经设置好参数，可遵照如下方法启动训练\n",
    "- (1) 在这个Notebook菜单栏上，点击Cell\n",
    "- (2) 在Cell下拉菜单里，点击 Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 21:30:59,054 : INFO : collecting all words and their counts\n",
      "2019-06-07 21:30:59,074 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-07 21:31:03,168 : INFO : PROGRESS: at sentence #10000, processed 116736 words, keeping 13165 word types\n",
      "2019-06-07 21:31:07,127 : INFO : PROGRESS: at sentence #20000, processed 236963 words, keeping 20263 word types\n",
      "2019-06-07 21:31:11,330 : INFO : PROGRESS: at sentence #30000, processed 354036 words, keeping 26195 word types\n",
      "2019-06-07 21:31:15,171 : INFO : PROGRESS: at sentence #40000, processed 468567 words, keeping 31382 word types\n",
      "2019-06-07 21:31:19,233 : INFO : PROGRESS: at sentence #50000, processed 585958 words, keeping 36301 word types\n",
      "2019-06-07 21:31:23,538 : INFO : PROGRESS: at sentence #60000, processed 713515 words, keeping 41646 word types\n",
      "2019-06-07 21:31:27,796 : INFO : PROGRESS: at sentence #70000, processed 837378 words, keeping 46367 word types\n",
      "2019-06-07 21:31:32,104 : INFO : PROGRESS: at sentence #80000, processed 956707 words, keeping 51101 word types\n",
      "2019-06-07 21:31:36,687 : INFO : PROGRESS: at sentence #90000, processed 1082478 words, keeping 56007 word types\n",
      "2019-06-07 21:31:38,176 : INFO : collected 57474 word types from a corpus of 1123711 raw words and 93401 sentences\n",
      "2019-06-07 21:31:38,177 : INFO : Loading a fresh vocabulary\n",
      "2019-06-07 21:31:38,208 : INFO : effective_min_count=5 retains 10092 unique words (17% of original 57474, drops 47382)\n",
      "2019-06-07 21:31:38,208 : INFO : effective_min_count=5 leaves 1059504 word corpus (94% of original 1123711, drops 64207)\n",
      "2019-06-07 21:31:38,238 : INFO : deleting the raw counts dictionary of 57474 items\n",
      "2019-06-07 21:31:38,241 : INFO : sample=0.001 downsamples 31 most-common words\n",
      "2019-06-07 21:31:38,242 : INFO : downsampling leaves estimated 846193 word corpus (79.9% of prior 1059504)\n",
      "2019-06-07 21:31:38,277 : INFO : estimated required memory for 10092 words and 100 dimensions: 13119600 bytes\n",
      "2019-06-07 21:31:38,278 : INFO : resetting layer weights\n",
      "c:\\programdata\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "2019-06-07 21:31:38,403 : INFO : training model with 3 workers on 10092 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-07 21:31:39,511 : INFO : EPOCH 1 - PROGRESS: at 4.15% examples, 34123 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:40,661 : INFO : EPOCH 1 - PROGRESS: at 8.94% examples, 33549 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:41,792 : INFO : EPOCH 1 - PROGRESS: at 13.76% examples, 33528 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:42,876 : INFO : EPOCH 1 - PROGRESS: at 18.22% examples, 33826 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:43,962 : INFO : EPOCH 1 - PROGRESS: at 22.57% examples, 33976 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:45,055 : INFO : EPOCH 1 - PROGRESS: at 26.98% examples, 34027 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:46,151 : INFO : EPOCH 1 - PROGRESS: at 31.55% examples, 34069 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:47,249 : INFO : EPOCH 1 - PROGRESS: at 36.28% examples, 34052 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:48,366 : INFO : EPOCH 1 - PROGRESS: at 41.16% examples, 34005 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:49,449 : INFO : EPOCH 1 - PROGRESS: at 45.72% examples, 34061 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:50,537 : INFO : EPOCH 1 - PROGRESS: at 50.15% examples, 34127 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:51,641 : INFO : EPOCH 1 - PROGRESS: at 54.74% examples, 34121 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:52,657 : INFO : EPOCH 1 - PROGRESS: at 58.54% examples, 34347 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:53,769 : INFO : EPOCH 1 - PROGRESS: at 62.99% examples, 34284 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:54,857 : INFO : EPOCH 1 - PROGRESS: at 67.54% examples, 34282 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:55,918 : INFO : EPOCH 1 - PROGRESS: at 71.90% examples, 34348 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:56,987 : INFO : EPOCH 1 - PROGRESS: at 75.86% examples, 34412 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:58,122 : INFO : EPOCH 1 - PROGRESS: at 80.76% examples, 34325 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:31:59,189 : INFO : EPOCH 1 - PROGRESS: at 85.05% examples, 34374 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:00,221 : INFO : EPOCH 1 - PROGRESS: at 89.02% examples, 34481 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:01,310 : INFO : EPOCH 1 - PROGRESS: at 93.36% examples, 34474 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:02,457 : INFO : EPOCH 1 - PROGRESS: at 97.90% examples, 34417 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:02,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-07 21:32:02,974 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-07 21:32:02,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-07 21:32:02,982 : INFO : EPOCH - 1 : training on 1123711 raw words (846184 effective words) took 24.6s, 34435 effective words/s\n",
      "2019-06-07 21:32:04,077 : INFO : EPOCH 2 - PROGRESS: at 4.15% examples, 34421 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:05,249 : INFO : EPOCH 2 - PROGRESS: at 8.94% examples, 33391 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:06,357 : INFO : EPOCH 2 - PROGRESS: at 13.76% examples, 33669 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:07,425 : INFO : EPOCH 2 - PROGRESS: at 18.22% examples, 34029 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:08,488 : INFO : EPOCH 2 - PROGRESS: at 22.57% examples, 34309 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:09,561 : INFO : EPOCH 2 - PROGRESS: at 26.98% examples, 34418 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:10,631 : INFO : EPOCH 2 - PROGRESS: at 31.55% examples, 34511 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:11,729 : INFO : EPOCH 2 - PROGRESS: at 36.28% examples, 34454 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:12,821 : INFO : EPOCH 2 - PROGRESS: at 41.16% examples, 34447 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:13,913 : INFO : EPOCH 2 - PROGRESS: at 45.72% examples, 34431 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:15,011 : INFO : EPOCH 2 - PROGRESS: at 50.15% examples, 34426 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:16,105 : INFO : EPOCH 2 - PROGRESS: at 54.74% examples, 34428 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:17,326 : INFO : EPOCH 2 - PROGRESS: at 59.62% examples, 34657 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:18,378 : INFO : EPOCH 2 - PROGRESS: at 63.88% examples, 34708 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:19,467 : INFO : EPOCH 2 - PROGRESS: at 68.47% examples, 34680 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:20,580 : INFO : EPOCH 2 - PROGRESS: at 72.65% examples, 34646 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:21,729 : INFO : EPOCH 2 - PROGRESS: at 76.93% examples, 34533 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:22,861 : INFO : EPOCH 2 - PROGRESS: at 81.66% examples, 34452 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:23,994 : INFO : EPOCH 2 - PROGRESS: at 85.84% examples, 34382 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:25,082 : INFO : EPOCH 2 - PROGRESS: at 89.84% examples, 34397 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:26,210 : INFO : EPOCH 2 - PROGRESS: at 94.31% examples, 34342 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:27,328 : INFO : EPOCH 2 - PROGRESS: at 98.75% examples, 34317 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:27,632 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-07 21:32:27,633 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-07 21:32:27,640 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-07 21:32:27,640 : INFO : EPOCH - 2 : training on 1123711 raw words (846504 effective words) took 24.7s, 34333 effective words/s\n",
      "2019-06-07 21:32:28,751 : INFO : EPOCH 3 - PROGRESS: at 4.15% examples, 33934 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 21:32:29,864 : INFO : EPOCH 3 - PROGRESS: at 8.94% examples, 34005 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:30,944 : INFO : EPOCH 3 - PROGRESS: at 13.76% examples, 34338 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:32,033 : INFO : EPOCH 3 - PROGRESS: at 18.22% examples, 34402 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:33,067 : INFO : EPOCH 3 - PROGRESS: at 22.57% examples, 34794 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:34,136 : INFO : EPOCH 3 - PROGRESS: at 26.98% examples, 34831 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:35,196 : INFO : EPOCH 3 - PROGRESS: at 31.55% examples, 34920 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:36,275 : INFO : EPOCH 3 - PROGRESS: at 36.28% examples, 34869 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:37,356 : INFO : EPOCH 3 - PROGRESS: at 41.16% examples, 34854 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:38,409 : INFO : EPOCH 3 - PROGRESS: at 45.72% examples, 34918 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:39,462 : INFO : EPOCH 3 - PROGRESS: at 50.15% examples, 35014 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:40,522 : INFO : EPOCH 3 - PROGRESS: at 54.74% examples, 35047 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:41,522 : INFO : EPOCH 3 - PROGRESS: at 58.54% examples, 35257 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:42,587 : INFO : EPOCH 3 - PROGRESS: at 62.99% examples, 35238 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:43,654 : INFO : EPOCH 3 - PROGRESS: at 67.54% examples, 35221 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:44,725 : INFO : EPOCH 3 - PROGRESS: at 71.90% examples, 35213 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:45,772 : INFO : EPOCH 3 - PROGRESS: at 75.86% examples, 35273 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:46,955 : INFO : EPOCH 3 - PROGRESS: at 80.76% examples, 35048 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:48,039 : INFO : EPOCH 3 - PROGRESS: at 85.05% examples, 35033 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:49,105 : INFO : EPOCH 3 - PROGRESS: at 89.02% examples, 35047 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:50,197 : INFO : EPOCH 3 - PROGRESS: at 93.36% examples, 35005 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:51,318 : INFO : EPOCH 3 - PROGRESS: at 97.90% examples, 34953 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:51,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-07 21:32:51,871 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-07 21:32:51,876 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-07 21:32:51,877 : INFO : EPOCH - 3 : training on 1123711 raw words (846030 effective words) took 24.2s, 34911 effective words/s\n",
      "2019-06-07 21:32:53,001 : INFO : EPOCH 4 - PROGRESS: at 4.15% examples, 33570 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:54,239 : INFO : EPOCH 4 - PROGRESS: at 8.94% examples, 32068 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:55,327 : INFO : EPOCH 4 - PROGRESS: at 13.76% examples, 32945 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:56,397 : INFO : EPOCH 4 - PROGRESS: at 18.22% examples, 33497 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:57,493 : INFO : EPOCH 4 - PROGRESS: at 22.57% examples, 33668 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:58,550 : INFO : EPOCH 4 - PROGRESS: at 26.98% examples, 33948 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:32:59,604 : INFO : EPOCH 4 - PROGRESS: at 31.55% examples, 34177 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:00,698 : INFO : EPOCH 4 - PROGRESS: at 36.28% examples, 34167 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:01,796 : INFO : EPOCH 4 - PROGRESS: at 41.16% examples, 34171 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:02,861 : INFO : EPOCH 4 - PROGRESS: at 45.72% examples, 34260 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:03,906 : INFO : EPOCH 4 - PROGRESS: at 50.15% examples, 34428 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:04,995 : INFO : EPOCH 4 - PROGRESS: at 54.74% examples, 34426 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:06,018 : INFO : EPOCH 4 - PROGRESS: at 58.54% examples, 34618 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:07,087 : INFO : EPOCH 4 - PROGRESS: at 62.99% examples, 34630 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:08,151 : INFO : EPOCH 4 - PROGRESS: at 67.54% examples, 34656 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:09,244 : INFO : EPOCH 4 - PROGRESS: at 71.90% examples, 34637 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:10,297 : INFO : EPOCH 4 - PROGRESS: at 75.86% examples, 34720 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:11,427 : INFO : EPOCH 4 - PROGRESS: at 80.76% examples, 34627 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:12,488 : INFO : EPOCH 4 - PROGRESS: at 85.05% examples, 34674 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:13,539 : INFO : EPOCH 4 - PROGRESS: at 89.02% examples, 34731 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:14,579 : INFO : EPOCH 4 - PROGRESS: at 93.36% examples, 34783 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:15,631 : INFO : EPOCH 4 - PROGRESS: at 97.90% examples, 34845 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:16,145 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-07 21:33:16,145 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-07 21:33:16,150 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-07 21:33:16,151 : INFO : EPOCH - 4 : training on 1123711 raw words (846108 effective words) took 24.3s, 34860 effective words/s\n",
      "2019-06-07 21:33:17,214 : INFO : EPOCH 5 - PROGRESS: at 4.15% examples, 35519 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:18,334 : INFO : EPOCH 5 - PROGRESS: at 8.94% examples, 34700 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:19,445 : INFO : EPOCH 5 - PROGRESS: at 13.76% examples, 34515 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:20,500 : INFO : EPOCH 5 - PROGRESS: at 18.22% examples, 34793 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:21,596 : INFO : EPOCH 5 - PROGRESS: at 22.57% examples, 34711 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:22,665 : INFO : EPOCH 5 - PROGRESS: at 26.98% examples, 34760 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:23,745 : INFO : EPOCH 5 - PROGRESS: at 31.55% examples, 34759 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:24,853 : INFO : EPOCH 5 - PROGRESS: at 36.28% examples, 34622 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:25,945 : INFO : EPOCH 5 - PROGRESS: at 41.16% examples, 34600 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:27,006 : INFO : EPOCH 5 - PROGRESS: at 45.72% examples, 34665 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:28,098 : INFO : EPOCH 5 - PROGRESS: at 50.15% examples, 34659 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:29,173 : INFO : EPOCH 5 - PROGRESS: at 54.74% examples, 34678 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:30,405 : INFO : EPOCH 5 - PROGRESS: at 59.62% examples, 34862 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:31,438 : INFO : EPOCH 5 - PROGRESS: at 63.88% examples, 34942 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:32,688 : INFO : EPOCH 5 - PROGRESS: at 68.47% examples, 34561 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:33,719 : INFO : EPOCH 5 - PROGRESS: at 71.90% examples, 34252 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:34,738 : INFO : EPOCH 5 - PROGRESS: at 75.10% examples, 34015 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:36,001 : INFO : EPOCH 5 - PROGRESS: at 79.86% examples, 33735 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:37,005 : INFO : EPOCH 5 - PROGRESS: at 83.36% examples, 33557 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:38,244 : INFO : EPOCH 5 - PROGRESS: at 87.39% examples, 33379 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:39,297 : INFO : EPOCH 5 - PROGRESS: at 90.78% examples, 33156 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:40,444 : INFO : EPOCH 5 - PROGRESS: at 94.31% examples, 32832 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-07 21:33:41,516 : INFO : EPOCH 5 - PROGRESS: at 97.90% examples, 32642 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 21:33:42,182 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-07 21:33:42,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-07 21:33:42,192 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-07 21:33:42,194 : INFO : EPOCH - 5 : training on 1123711 raw words (846375 effective words) took 26.0s, 32503 effective words/s\n",
      "2019-06-07 21:33:42,195 : INFO : training on a 5618555 raw words (4231201 effective words) took 123.8s, 34180 effective words/s\n",
      "2019-06-07 21:33:42,195 : INFO : saving Word2Vec object under teccl.model, separately None\n",
      "2019-06-07 21:33:42,197 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-06-07 21:33:42,306 : INFO : saved teccl.model\n"
     ]
    }
   ],
   "source": [
    "model = train_w2vmodel(mycorpus,saved_path,**paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下一步 \n",
    "- 词向量加载\n",
    "- 词向量相似词查询\n",
    "- 相似度计算\n",
    "- 类比推理\n",
    "- 可视化呈现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
