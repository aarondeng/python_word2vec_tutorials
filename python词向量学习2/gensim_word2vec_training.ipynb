{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备语料\n",
    "- 以TECCL语料库为例\n",
    "- Ten-thousand English Compositions of Chinese Learners (the TECCL Corpus)，中国学生万篇英语作文语料库V1.1，由北外许家金教授创建的开放获取学习者语料库。网址：http://corpus.bfsu.edu.cn/content/teccl-corpus\n",
    "- 下载后解压\n",
    "- 把其中的“01TECCL_V1.1_RAW”文件夹包括其中的文件拷贝到当前目录\n",
    "- **注意** 这个语料的规模对于词向量训练是不够的，仅做演示之用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"red\",size=4>**只需在这部分设置参数、语料地址、模型保存地址以及语料库预处理定制**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量参数\n",
    "- 根据需要修改下面的参数，注意保留每一行后面的英文逗号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>size</b>:  Dimensionality of the word vectors. 向量维度\n",
    "- <b>window</b>: Maximum distance between the current and predicted word within a sentence.窗口大小\n",
    "- <b>min_count</b>: Ignores all words with total frequency lower than this. 最小词频\n",
    "- <b>iter</b>: Number of iterations (epochs) over the corpus. 训练次数\n",
    "- <b>sg</b>:  Training algorithm:{0, 1} 1 for skip-gram; otherwise CBOW.训练算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = {\n",
    "     \"size\" : 100,\n",
    "     \"window\" : 5,\n",
    "    \"min_count\" : 5,\n",
    "    \"iter\": 5, \n",
    "    \"sg\":1,\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料库地址以及词向量模型保存地址\n",
    "- 文件或目录地址要用反斜杠“/”分割路径\n",
    "- 放在英文模式引号里面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 语料库地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"01TECCL_V1.1_RAW\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 词向量模型保存地址\n",
    "- 若扩展名为“bin”或者“txt”,则保存为谷歌原word2vec工具保存的通用格式\n",
    "- 若扩展名为“model”或者其它，则保存为Gensim特有的词向量保存格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_path = \"teccl.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料预处理设置\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否分句： 是， True; 否, False\n",
    "- 如果语料是每行一句，一般可以选否；否则根据需要是否将每一个段落进行分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否分词： 是， True; 否, False\n",
    "- 如果语料中每个单词及标点已经由空格分隔，选否；否则根据需要是否将每个句子进行分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否统一为小写： 是，True； 否，False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 是否去除停用词： 是， True; 否, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopwords = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 默认调用nltk的停用词列表，即None\n",
    "- 若需要更改，请填入停用词列表， 如：\n",
    "```python\n",
    "stopwords = ['is','a','the','an']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,re\n",
    "import nltk\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models import utils_any2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):    \n",
    "    def __init__(self,corp_dir = None,suffixes =None,**kwargs):\n",
    "        self.fdir = corp_dir\n",
    "        suffixes = [suffixes] if isinstance(suffixes,str) else suffixes\n",
    "        self.suffixes = suffixes if suffixes else ['.txt']\n",
    "        self.files = self._get_files(self.fdir)\n",
    "        self.kwargs = kwargs\n",
    "        if kwargs['remove_stopwords']:\n",
    "            if not kwargs['stopwords']:\n",
    "                from nltk.corpus import stopwords\n",
    "                try:\n",
    "                    self.stopwords = stopwords.words('english')\n",
    "                except:\n",
    "                    nltk.download('stopwords')\n",
    "                    self.stopwords = stopwords.words('english')\n",
    "            else:\n",
    "                self.stopwords = kwargs['stopwords']\n",
    "        \n",
    "    def _get_files(self,path):\n",
    "        files = []\n",
    "        for f in os.listdir(path):\n",
    "            fpath = os.path.join(path,f)\n",
    "            if os.path.isdir(fpath): continue            \n",
    "            if self._check_file_type(f):\n",
    "                files.append(fpath)\n",
    "        logging.info('%d file(s) loaded!'%len(files))\n",
    "        return files\n",
    "                    \n",
    "    def _check_file_type(self,f):\n",
    "        for suffix in self.suffixes:\n",
    "            if f.endswith(suffix):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def __iter__(self):\n",
    "        for f in self.files:\n",
    "            for line in open(f,encoding='utf-8'):\n",
    "                line = line.strip()                \n",
    "                if not line: continue                          \n",
    "                for words in self.preprocess(line):\n",
    "                    yield words\n",
    "                    \n",
    "    def preprocess(self,line):\n",
    "        sents = nltk.tokenize.sent_tokenize(line) if self.kwargs['sent_tokenize'] else [line]\n",
    "        for sent in sents:\n",
    "            words = nltk.tokenize.word_tokenize(sent) if self.kwargs['word_tokenize'] else sent.split()\n",
    "            out_words = []\n",
    "            for w in words:\n",
    "                if not w.strip(): continue  \n",
    "                if self.kwargs['lower_case']:\n",
    "                    w = w.lower()\n",
    "                if self.kwargs['remove_stopwords']:\n",
    "                    if w in self.stopwords:\n",
    "                        continue\n",
    "                out_words.append(w)\n",
    "            \n",
    "            yield out_words\n",
    "\n",
    "def gensim2wordvec(model,w2v_path):        \n",
    "    dels = [w for w in model.wv.vocab if ' 'in w]\n",
    "    for w in dels: del model.wv.vocab[w]\n",
    "    vectors = []\n",
    "    i = 0\n",
    "    for w in model.wv.vocab:\n",
    "        vectors.append(model.wv[w])\n",
    "        model.wv.vocab[w].index = i\n",
    "        i += 1\n",
    "    vectors = np.array(vectors)\n",
    "    binary = True if w2v_path.endswith('.bin') else False\n",
    "    utils_any2vec._save_word2vec_format(w2v_path, model.wv.vocab,vectors,binary=binary)\n",
    "\n",
    "def train_w2vmodel(sentences,save2path,**kwargs):     \n",
    "    model = gensim.models.Word2Vec(**kwargs)   \n",
    "    model.build_vocab(sentences)\n",
    "    model.train(sentences,total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    if save2path.endswith('.txt') or save2path.endswith('.bin'):\n",
    "        gensim2wordvec(model,save2path)\n",
    "    else:\n",
    "        model.save(save2path,ignore=[])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-26 10:13:20,205 : INFO : 9864 file(s) loaded!\n"
     ]
    }
   ],
   "source": [
    "mycorpus = Corpus(corpus_path,suffixes=['.txt'],\n",
    "                          sent_tokenize = sent_tokenize,\n",
    "                         word_tokenize = word_tokenize,\n",
    "                         lower_case = lower_case,\n",
    "                         remove_stopwords = remove_stopwords,\n",
    "                         stopwords= stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=\"red\",size=3>**启动词向量训练**</font>\n",
    "- 若已经设置好参数，可遵照如下方法启动训练\n",
    "- (1) 在这个Notebook菜单栏上，点击Cell\n",
    "- (2) 在Cell下拉菜单里，点击 Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-26 10:13:20,237 : INFO : collecting all words and their counts\n",
      "2019-06-26 10:13:20,252 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-06-26 10:13:23,690 : INFO : PROGRESS: at sentence #10000, processed 208368 words, keeping 13300 word types\n",
      "2019-06-26 10:13:27,018 : INFO : PROGRESS: at sentence #20000, processed 419787 words, keeping 20400 word types\n",
      "2019-06-26 10:13:30,769 : INFO : PROGRESS: at sentence #30000, processed 626892 words, keeping 26334 word types\n",
      "2019-06-26 10:13:34,019 : INFO : PROGRESS: at sentence #40000, processed 832074 words, keeping 31523 word types\n",
      "2019-06-26 10:13:37,176 : INFO : PROGRESS: at sentence #50000, processed 1039883 words, keeping 36442 word types\n",
      "2019-06-26 10:13:40,489 : INFO : PROGRESS: at sentence #60000, processed 1263703 words, keeping 41787 word types\n",
      "2019-06-26 10:13:43,927 : INFO : PROGRESS: at sentence #70000, processed 1484104 words, keeping 46508 word types\n",
      "2019-06-26 10:13:47,521 : INFO : PROGRESS: at sentence #80000, processed 1695947 words, keeping 51243 word types\n",
      "2019-06-26 10:13:51,412 : INFO : PROGRESS: at sentence #90000, processed 1917102 words, keeping 56149 word types\n",
      "2019-06-26 10:13:52,615 : INFO : collected 57616 word types from a corpus of 1990083 raw words and 93401 sentences\n",
      "2019-06-26 10:13:52,615 : INFO : Loading a fresh vocabulary\n",
      "2019-06-26 10:13:52,724 : INFO : effective_min_count=5 retains 10227 unique words (17% of original 57616, drops 47389)\n",
      "2019-06-26 10:13:52,724 : INFO : effective_min_count=5 leaves 1925866 word corpus (96% of original 1990083, drops 64217)\n",
      "2019-06-26 10:13:52,771 : INFO : deleting the raw counts dictionary of 57616 items\n",
      "2019-06-26 10:13:52,771 : INFO : sample=0.001 downsamples 58 most-common words\n",
      "2019-06-26 10:13:52,771 : INFO : downsampling leaves estimated 1326531 word corpus (68.9% of prior 1925866)\n",
      "2019-06-26 10:13:52,802 : INFO : estimated required memory for 10227 words and 100 dimensions: 13295100 bytes\n",
      "2019-06-26 10:13:52,802 : INFO : resetting layer weights\n",
      "2019-06-26 10:13:52,912 : INFO : training model with 3 workers on 10227 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-26 10:13:54,006 : INFO : EPOCH 1 - PROGRESS: at 5.19% examples, 67314 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:13:55,053 : INFO : EPOCH 1 - PROGRESS: at 10.78% examples, 65564 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:13:56,069 : INFO : EPOCH 1 - PROGRESS: at 16.08% examples, 65897 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:13:57,147 : INFO : EPOCH 1 - PROGRESS: at 21.35% examples, 66225 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:13:58,163 : INFO : EPOCH 1 - PROGRESS: at 26.42% examples, 66236 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:13:59,257 : INFO : EPOCH 1 - PROGRESS: at 32.25% examples, 66264 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:00,335 : INFO : EPOCH 1 - PROGRESS: at 37.95% examples, 66369 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:01,413 : INFO : EPOCH 1 - PROGRESS: at 43.68% examples, 66608 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:02,444 : INFO : EPOCH 1 - PROGRESS: at 48.88% examples, 66423 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:03,523 : INFO : EPOCH 1 - PROGRESS: at 54.49% examples, 66500 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:04,585 : INFO : EPOCH 1 - PROGRESS: at 59.56% examples, 66795 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:05,648 : INFO : EPOCH 1 - PROGRESS: at 64.90% examples, 66879 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:06,695 : INFO : EPOCH 1 - PROGRESS: at 70.13% examples, 66661 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:07,710 : INFO : EPOCH 1 - PROGRESS: at 74.63% examples, 66616 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:08,726 : INFO : EPOCH 1 - PROGRESS: at 79.31% examples, 66116 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:09,773 : INFO : EPOCH 1 - PROGRESS: at 84.32% examples, 65934 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:10,804 : INFO : EPOCH 1 - PROGRESS: at 88.87% examples, 65850 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:11,851 : INFO : EPOCH 1 - PROGRESS: at 93.25% examples, 65351 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:12,867 : INFO : EPOCH 1 - PROGRESS: at 97.91% examples, 65056 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:13,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 10:14:13,289 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 10:14:13,305 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 10:14:13,305 : INFO : EPOCH - 1 : training on 1990083 raw words (1326610 effective words) took 20.4s, 65086 effective words/s\n",
      "2019-06-26 10:14:14,352 : INFO : EPOCH 2 - PROGRESS: at 4.65% examples, 63927 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:15,414 : INFO : EPOCH 2 - PROGRESS: at 10.18% examples, 63202 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:16,461 : INFO : EPOCH 2 - PROGRESS: at 15.55% examples, 63453 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:17,539 : INFO : EPOCH 2 - PROGRESS: at 20.37% examples, 63158 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:18,649 : INFO : EPOCH 2 - PROGRESS: at 25.47% examples, 62485 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:19,696 : INFO : EPOCH 2 - PROGRESS: at 30.37% examples, 62549 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:20,789 : INFO : EPOCH 2 - PROGRESS: at 35.68% examples, 62200 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:21,868 : INFO : EPOCH 2 - PROGRESS: at 41.18% examples, 62224 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:22,915 : INFO : EPOCH 2 - PROGRESS: at 46.38% examples, 62393 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:23,993 : INFO : EPOCH 2 - PROGRESS: at 51.46% examples, 62251 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:25,087 : INFO : EPOCH 2 - PROGRESS: at 56.34% examples, 62194 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:26,087 : INFO : EPOCH 2 - PROGRESS: at 60.75% examples, 62461 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:27,149 : INFO : EPOCH 2 - PROGRESS: at 65.90% examples, 62448 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:28,212 : INFO : EPOCH 2 - PROGRESS: at 70.63% examples, 62041 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:29,306 : INFO : EPOCH 2 - PROGRESS: at 75.10% examples, 61943 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:30,400 : INFO : EPOCH 2 - PROGRESS: at 80.45% examples, 61919 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:31,400 : INFO : EPOCH 2 - PROGRESS: at 85.21% examples, 62171 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:32,415 : INFO : EPOCH 2 - PROGRESS: at 89.75% examples, 62345 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:33,509 : INFO : EPOCH 2 - PROGRESS: at 94.76% examples, 62260 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:34,556 : INFO : EPOCH 2 - PROGRESS: at 99.78% examples, 62331 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:34,572 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 10:14:34,572 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 10:14:34,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 10:14:34,587 : INFO : EPOCH - 2 : training on 1990083 raw words (1326794 effective words) took 21.3s, 62342 effective words/s\n",
      "2019-06-26 10:14:35,609 : INFO : EPOCH 3 - PROGRESS: at 4.15% examples, 59022 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:36,656 : INFO : EPOCH 3 - PROGRESS: at 9.54% examples, 61181 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:37,687 : INFO : EPOCH 3 - PROGRESS: at 14.93% examples, 62675 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:38,687 : INFO : EPOCH 3 - PROGRESS: at 19.84% examples, 63446 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:39,719 : INFO : EPOCH 3 - PROGRESS: at 24.98% examples, 63731 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:40,718 : INFO : EPOCH 3 - PROGRESS: at 29.78% examples, 64102 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:41,750 : INFO : EPOCH 3 - PROGRESS: at 35.11% examples, 64107 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:42,844 : INFO : EPOCH 3 - PROGRESS: at 40.72% examples, 63690 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-26 10:14:43,907 : INFO : EPOCH 3 - PROGRESS: at 45.82% examples, 63540 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:44,907 : INFO : EPOCH 3 - PROGRESS: at 50.72% examples, 63829 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:45,938 : INFO : EPOCH 3 - PROGRESS: at 55.87% examples, 63974 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:47,016 : INFO : EPOCH 3 - PROGRESS: at 60.75% examples, 64220 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:48,032 : INFO : EPOCH 3 - PROGRESS: at 65.90% examples, 64270 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:49,048 : INFO : EPOCH 3 - PROGRESS: at 71.04% examples, 64377 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:50,110 : INFO : EPOCH 3 - PROGRESS: at 75.97% examples, 64681 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:51,220 : INFO : EPOCH 3 - PROGRESS: at 81.34% examples, 64454 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:52,220 : INFO : EPOCH 3 - PROGRESS: at 86.15% examples, 64530 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:53,329 : INFO : EPOCH 3 - PROGRESS: at 91.35% examples, 64655 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:54,329 : INFO : EPOCH 3 - PROGRESS: at 96.31% examples, 64725 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:55,048 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 10:14:55,048 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 10:14:55,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 10:14:55,079 : INFO : EPOCH - 3 : training on 1990083 raw words (1326445 effective words) took 20.5s, 64786 effective words/s\n",
      "2019-06-26 10:14:56,079 : INFO : EPOCH 4 - PROGRESS: at 4.65% examples, 65831 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:57,173 : INFO : EPOCH 4 - PROGRESS: at 10.18% examples, 63525 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:58,205 : INFO : EPOCH 4 - PROGRESS: at 15.55% examples, 64080 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:14:59,236 : INFO : EPOCH 4 - PROGRESS: at 20.37% examples, 64189 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:00,252 : INFO : EPOCH 4 - PROGRESS: at 25.47% examples, 64464 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:01,377 : INFO : EPOCH 4 - PROGRESS: at 30.96% examples, 64526 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:02,408 : INFO : EPOCH 4 - PROGRESS: at 36.27% examples, 64477 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:03,424 : INFO : EPOCH 4 - PROGRESS: at 41.56% examples, 64616 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:04,455 : INFO : EPOCH 4 - PROGRESS: at 46.86% examples, 64597 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:05,486 : INFO : EPOCH 4 - PROGRESS: at 51.87% examples, 64572 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:06,565 : INFO : EPOCH 4 - PROGRESS: at 57.03% examples, 64928 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:07,643 : INFO : EPOCH 4 - PROGRESS: at 62.34% examples, 65123 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:08,705 : INFO : EPOCH 4 - PROGRESS: at 67.60% examples, 64909 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:09,768 : INFO : EPOCH 4 - PROGRESS: at 72.80% examples, 65200 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:10,877 : INFO : EPOCH 4 - PROGRESS: at 78.20% examples, 65265 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:11,893 : INFO : EPOCH 4 - PROGRESS: at 83.32% examples, 65305 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:13,002 : INFO : EPOCH 4 - PROGRESS: at 88.37% examples, 65384 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:14,002 : INFO : EPOCH 4 - PROGRESS: at 93.25% examples, 65363 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:15,018 : INFO : EPOCH 4 - PROGRESS: at 98.37% examples, 65397 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:15,331 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 10:15:15,331 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 10:15:15,362 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 10:15:15,362 : INFO : EPOCH - 4 : training on 1990083 raw words (1327071 effective words) took 20.3s, 65447 effective words/s\n",
      "2019-06-26 10:15:16,456 : INFO : EPOCH 5 - PROGRESS: at 5.19% examples, 66382 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:17,518 : INFO : EPOCH 5 - PROGRESS: at 10.78% examples, 64532 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:18,534 : INFO : EPOCH 5 - PROGRESS: at 16.08% examples, 65143 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:19,628 : INFO : EPOCH 5 - PROGRESS: at 21.35% examples, 65573 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:20,659 : INFO : EPOCH 5 - PROGRESS: at 26.42% examples, 65350 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:21,659 : INFO : EPOCH 5 - PROGRESS: at 31.53% examples, 65399 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:22,659 : INFO : EPOCH 5 - PROGRESS: at 36.71% examples, 65441 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:23,691 : INFO : EPOCH 5 - PROGRESS: at 42.09% examples, 65430 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:24,722 : INFO : EPOCH 5 - PROGRESS: at 47.38% examples, 65340 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:25,738 : INFO : EPOCH 5 - PROGRESS: at 52.37% examples, 65384 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:26,800 : INFO : EPOCH 5 - PROGRESS: at 57.47% examples, 65663 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:27,816 : INFO : EPOCH 5 - PROGRESS: at 62.34% examples, 65639 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:28,894 : INFO : EPOCH 5 - PROGRESS: at 67.60% examples, 65235 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:29,910 : INFO : EPOCH 5 - PROGRESS: at 72.38% examples, 65341 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:30,988 : INFO : EPOCH 5 - PROGRESS: at 77.57% examples, 65514 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:32,019 : INFO : EPOCH 5 - PROGRESS: at 82.74% examples, 65461 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:33,113 : INFO : EPOCH 5 - PROGRESS: at 87.88% examples, 65532 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:34,113 : INFO : EPOCH 5 - PROGRESS: at 92.77% examples, 65542 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:35,145 : INFO : EPOCH 5 - PROGRESS: at 97.91% examples, 65510 words/s, in_qsize 0, out_qsize 0\n",
      "2019-06-26 10:15:35,551 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-26 10:15:35,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-26 10:15:35,566 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-26 10:15:35,566 : INFO : EPOCH - 5 : training on 1990083 raw words (1326004 effective words) took 20.2s, 65605 effective words/s\n",
      "2019-06-26 10:15:35,566 : INFO : training on a 9950415 raw words (6632924 effective words) took 102.6s, 64617 effective words/s\n",
      "2019-06-26 10:15:35,598 : INFO : storing 10227x100 projection weights into teccl.txt\n",
      "2019-06-26 10:15:35,598 : WARNING : this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "model = train_w2vmodel(mycorpus,saved_path,**paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下一步 \n",
    "- 词向量加载\n",
    "- 词向量相似词查询\n",
    "- 相似度计算\n",
    "- 类比推理\n",
    "- 可视化呈现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
